Below is your **complete, polished, professional, GitHub-ready README.md** â€” fully formatted, clear, and detailed.
You can copyâ€“paste directly into GitHub.

---

# âš¡ **Energy-Aware System Monitor**

A comprehensive and efficient system monitoring framework that tracks **CPU usage**, **network packets**, **system calls**, **context switches**, and provides a **software-based energy consumption estimation**.

This project includes **two implementations**:

* **eBPF-based monitor** â€” high-accuracy kernel-level monitoring
* **Traditional baseline monitor** â€” user-space monitoring using standard Linux tools

---

## ğŸ“˜ **Overview**

Modern systems generate millions of events per second. Traditional monitoring tools rely on heavy user-space polling, which introduces **latency**, **CPU overhead**, and **extra energy consumption**.

This project demonstrates how **eBPF** can drastically improve:

* Monitoring accuracy
* Latency
* System efficiency
* Energy consumption

Both monitors produce CSV logs that can be used for further research, visualization, or performance comparison.

---

# ğŸ“‹ **What This Tool Monitors**

This system monitors:

### ğŸ”¹ **CPU usage**

Measured once every second using psutil.

### ğŸ”¹ **Network packets**

Tracks both received & transmitted packets.

### ğŸ”¹ **System calls**

Captures how frequently user applications request services from the kernel.

### ğŸ”¹ **Context switches**

Measures how often the CPU switches between processes or threads.

### ğŸ”¹ **Software-based energy estimation**

A formula calculates approximate energy consumption:

```
Energy(J) = (CPU% * 0.4) + (Syscalls * 0.00005) + (CtxSwitch * 0.0002)
```

---

# ğŸ”§ **Project Components**

## 1ï¸âƒ£ **eBPF Monitor (`ebpf_monitor.py`)**

A powerful monitoring script using **Extended Berkeley Packet Filter (eBPF)**.

### âœ” Features

* Real-time kernel-level monitoring
* Tracks system calls, context switches, and packets via kernel tracepoints
* Automatic traffic control (HTB + SFQ) based on energy usage
* Status indicators:

  * ğŸŸ¢ **NORMAL** â€” Energy < 6J
  * ğŸŸ¡ **ELEVATED** â€” 6â€“8J
  * ğŸŸ  **HIGH** â€” 8â€“10J
  * ğŸ”´ **CRITICAL** â€” >10J
* CSV logging (`system_monitor_log.csv`)
* Computes **average syscalls** before exit
* Extremely low latency (microseconds)

---

## 2ï¸âƒ£ **Traditional Baseline Monitor (`traditional_monitor.py`)**

A simpler fallback version that does **not require eBPF**.

### âœ” Features

* Portable â€” works on almost any Linux system
* Reads average syscalls from eBPF logs for fair comparison
* Uses `/proc/stat` + psutil
* CSV logging (`baseline_monitor_log.csv`)
* Easy to understand for beginners
* Higher latency compared to eBPF

---

# ğŸ§ª **Latency Comparison**

### âš¡ **eBPF Monitoring**

* Runs **inside kernel**, not user-space
* Captures events instantly
* **Microsecond-level latency**
* Ideal for high-speed networks & real-time systems

### ğŸŒ **Traditional Monitoring**

* User-space polling (1 second interval)
* Requires kernel â†’ user transitions
* Cannot directly measure syscalls
* **Millisecond to second-level latency**
* Slower response to rapid system changes

---

## For Traditional Monitor

* Any Linux system
* Python **3.x**
* Install:

```bash
pip3 install psutil
```

---

# â–¶ï¸ **How to Run the eBPF Monitor**

### 1. Make executable:

```bash
chmod +x ebpf_monitor.py
```

### 2. Run with sudo:

```bash
sudo ./ebpf_monitor.py
```

### 3. Stop monitoring:

Press **Ctrl + C**
The script will:

* Stop monitoring
* Compute average syscalls
* Update CSV
* Clean up traffic control settings

CSV saved as:

```
system_monitor_log.csv
```

---

# â–¶ï¸ **How to Run the Traditional Monitor**

Run:

```bash
python3 traditional_monitor.py
```

It will:

* Load average syscalls from eBPF log (if available)
* Begin logging data every second

CSV saved as:

```
baseline_monitor_log.csv
```

---

# ğŸ“ **Output Files**

### ğŸ“„ `system_monitor_log.csv` (eBPF Monitor)

Contains:

* timestamp
* status
* cpu
* packets
* syscalls
* context switches
* energy
* average syscalls

### ğŸ“„ `baseline_monitor_log.csv` (Traditional Monitor)

Contains:

* timestamp
* cpu
* packets
* delta_packets
* syscalls
* ctxswitches
* energy

---

# ğŸ”„ **Comparing Both Approaches**

Run in this order:

1. **eBPF monitor first**
2. **Traditional monitor second**

Traditional monitor reads the syscall average from the eBPF log.

### What you can compare:

* Kernel vs user-space latency
* Measurement accuracy
* CPU overhead
* Energy consumption
* Packet & context-switch detection quality

---

# ğŸ“‚ **Repository Structure**

```
ğŸ“‚ Repository Structure
â”œâ”€â”€ ebpf_monitor.py               # eBPF-based monitoring script
â”œâ”€â”€ traditional_monitor.py        # Baseline monitoring script
â”œâ”€â”€ system_monitor_log.csv        # Output from eBPF monitor
â”œâ”€â”€ baseline_monitor_log.csv      # Output from traditional monitor
â””â”€â”€ README.md                     # Documentation
```

---

ğŸ“¶ Dynamic Bandwidth Management (HTB + SFQ)

The eBPF monitor includes an automatic bandwidth control system using Linux Traffic Control (tc) to prevent high energy consumption during heavy loads.

When the system energy level goes high, the script does NOT reduce bandwidth, but instead classifies the current energy state and can trigger bandwidth policies.
Right now the project sets up a clean HTB (Hierarchical Token Bucket) environment for stable packet monitoring.

âœ” What the Bandwidth System Does

Removes any existing qdisc root (cleans the interface)

Creates a new HTB root qdisc

Creates a default traffic class at full speed (1000mbit)

Adds SFQ (Stochastic Fairness Queueing) to keep packet flow fair and stable

Ensures consistent and accurate packet sampling for eBPF

âœ” Why This Matters

Normal network interfaces have unpredictable buffering and queueing.
To get accurate packet counts for energy analysis, we apply:

HTB â†’ For shaping total bandwidth (fixed predictable rate)
SFQ â†’ For fair distribution of packets to avoid burst spikes

This helps ensure:

Stable Pkt/s (packet per second) measurement

Smoother energy calculation

More accurate traffic monitoring results

# ğŸ **Conclusion**

This project demonstrates the **clear advantage** of eBPF over traditional monitoring systems:

### With eBPF:

* Lower overhead
* Higher accuracy
* Better latency
* More complete system visibility
* More realistic energy estimation

Traditional monitoring is simpler but cannot match kernel-level precision.

---

